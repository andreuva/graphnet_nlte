{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3fa4e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.loader import DataLoader\n",
    "from tqdm import tqdm\n",
    "from graphnet import *\n",
    "from configobj import ConfigObj\n",
    "from Dataset import *\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b94eff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu = 3\n",
    "\n",
    "# Check if CUDA is available\n",
    "cuda_available = torch.cuda.is_available()\n",
    "\n",
    "# Set device to GPU 0 if available, otherwise CPU\n",
    "device = torch.device(f\"cuda:{gpu}\" if cuda_available else \"cpu\")\n",
    "\n",
    "# Print device info\n",
    "print(f\"CUDA available: {cuda_available}\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "113c9cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  LOAD THE DATACUBES OF THE GRID FROM C PORTA CODE\n",
    "\n",
    "datadir = '../data_porta'\n",
    "# ---- grid dimensions taken from the C code ----\n",
    "nx = ny = 504\n",
    "nz = 476 - 52 + 1          # 425\n",
    "nlev = 6                   # caii[0] … caii[5]\n",
    "\n",
    "# ---- memory–mapped array: reads only the chunks you touch ----\n",
    "pops = np.memmap(\n",
    "    f'{datadir}/AR_385_CaII_5L_pops.dat',\n",
    "    dtype='<f4',           # little-endian 32-bit float\n",
    "    mode='r',\n",
    "    shape=(nz, ny, nx, nlev)   # (k, j, i, level)==(z, y, x, L)\n",
    ")\n",
    "\n",
    "# ---- memory–mapped array: reads only the chunks you touch ----\n",
    "b_xyz = np.memmap(\n",
    "    f'{datadir}/AR_385_B.dat',\n",
    "    dtype='<f4',           # little-endian 32-bit float\n",
    "    mode='r',\n",
    "    shape=(nz, ny, nx, 3)   # (k, j, i, B_i)==(z, y, x, B)\n",
    ")\n",
    "\n",
    "# ---- memory–mapped array: reads only the chunks you touch ----\n",
    "temp = np.memmap(\n",
    "    f'{datadir}/AR_385_temp.dat',\n",
    "    dtype='<f4',           # little-endian 32-bit float\n",
    "    mode='r',\n",
    "    shape=(nz, ny, nx, 1)   # (k, j, i, 1)==(z, y, x, 1)\n",
    ")\n",
    "\n",
    "# ---- memory–mapped array: reads only the chunks you touch ----\n",
    "vel = np.memmap(\n",
    "    f'{datadir}/AR_385_veloc.dat',\n",
    "    dtype='<f4',           # little-endian 32-bit float\n",
    "    mode='r',\n",
    "    shape=(nz, ny, nx, 3)   # (k, j, i, 3)==(z, y, x, 3)\n",
    ")\n",
    "\n",
    "# ---- memory–mapped array: reads only the chunks you touch ----\n",
    "n_e = np.memmap(\n",
    "    f'{datadir}/AR_385_ne.dat',\n",
    "    dtype='<f4',           # little-endian 32-bit float\n",
    "    mode='r',\n",
    "    shape=(nz, ny, nx, 1)   # (k, j, i, 1)==(z, y, x, 1)\n",
    ")\n",
    "\n",
    "# ---- memory–mapped array: reads only the chunks you touch ----\n",
    "n_p = np.memmap(\n",
    "    f'{datadir}/AR_385_np.dat',\n",
    "    dtype='<f4',           # little-endian 32-bit float\n",
    "    mode='r',\n",
    "    shape=(nz, ny, nx, 1)   # (k, j, i, 1)==(z, y, x, 1)\n",
    ")\n",
    "\n",
    "# ---- memory–mapped array: reads only the chunks you touch ----\n",
    "n_h = np.memmap(\n",
    "    f'{datadir}/AR_385_nh.dat',\n",
    "    dtype='<f4',           # little-endian 32-bit float\n",
    "    mode='r',\n",
    "    shape=(nz, ny, nx, 1)   # (k, j, i, 1)==(z, y, x, 1)\n",
    ")\n",
    "\n",
    "# ---- memory–mapped array: reads only the chunks you touch ----\n",
    "# geom = np.memmap(\n",
    "#     f'{datadir}/AR_385_GEOMETRY.dat',\n",
    "#     dtype='<f4',           # little-endian 32-bit float\n",
    "#     mode='r',\n",
    "#     shape=(nz, ny, nx, 1)   # (k, j, i, 1)==(z, y, x, 1)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9bdf8aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Populations shape:\\t', pops.shape)\n",
    "print('Temperature shape:\\t', temp.shape)\n",
    "print('Mag, field shape:\\t', b_xyz.shape)\n",
    "print('Velocity shape:\\t\\t', vel.shape)\n",
    "print('N_elec shape:\\t\\t', n_e.shape)\n",
    "print('N_nh shape:\\t\\t', n_h.shape)\n",
    "print('N_p shape:\\t\\t', n_p.shape)\n",
    "\n",
    "from scipy.interpolate import interpn\n",
    "\n",
    "# Define a new, higher-resolution grid\n",
    "z, y, x = (np.arange(d) for d in (nz, ny, nx))\n",
    "\n",
    "new_nz, new_ny, new_nx = 128, nx, ny\n",
    "new_z, new_y, new_x = (np.linspace(0, d-1, new_d) for d, new_d in zip((nz,ny,nx), (new_nz,new_ny,new_nx)))\n",
    "new_zv, new_yv, new_xv = np.meshgrid(new_z, new_y, new_x, indexing='ij', sparse=True)\n",
    "\n",
    "# print('\\n')\n",
    "# print(f'original indices: {z}')\n",
    "# print(f'new indices: {new_z}')\n",
    "\n",
    "# Interpolate data onto the new grid\n",
    "new_points = (new_zv, new_yv, new_xv)\n",
    "pops_interp = interpn((z, y, x), pops, new_points)\n",
    "temp_interp = interpn((z, y, x), temp, new_points)\n",
    "b_xyz_interp = interpn((z, y, x), b_xyz, new_points)\n",
    "vel_interp = interpn((z, y, x), vel, new_points)\n",
    "n_e_interp = interpn((z, y, x), n_e, new_points)\n",
    "n_h_interp = interpn((z, y, x), n_h, new_points)\n",
    "n_p_interp = interpn((z, y, x), n_p, new_points)\n",
    "\n",
    "print('\\n'+'#'*60)\n",
    "print('Populations shape INTERPOLATED:\\t', pops_interp.shape)\n",
    "print('Temperature shape INTERPOLATED:\\t', temp_interp.shape)\n",
    "print('Mag, field shape INTERPOLATED:\\t', b_xyz_interp.shape)\n",
    "print('Velocity shape INTERPOLATED:\\t', vel_interp.shape)\n",
    "print('N_elec shape INTERPOLATED:\\t', n_e_interp.shape)\n",
    "print('N_nh shape INTERPOLATED:\\t', n_h_interp.shape)\n",
    "print('N_p shape INTERPOLATED:\\t\\t', n_p_interp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4219ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pops = pops_interp\n",
    "temp = temp_interp\n",
    "b_xyz = b_xyz_interp\n",
    "vel = vel_interp\n",
    "n_e = n_e_interp\n",
    "n_h = n_h_interp\n",
    "n_p = n_p_interp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90b10726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the configuration file\n",
    "config_file = 'conf.dat'\n",
    "with open(config_file, 'r') as f:\n",
    "    tmp = f.readlines()\n",
    "    f.close()\n",
    "\n",
    "    # Parse configuration file and transform to integers\n",
    "    hyperparameters = ConfigObj(tmp)\n",
    "\n",
    "for k, q in hyperparameters.items():\n",
    "    hyperparameters[k] = int(q)\n",
    "\n",
    "# Instantiate the model with the hyperparameters\n",
    "model = EncodeProcessDecode(**hyperparameters).to(device)\n",
    "# Print the number of trainable parameters\n",
    "print('N. total trainable parameters : {0}'.format(sum(p.numel() for p in model.parameters() if p.requires_grad)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f9b1129",
   "metadata": {},
   "outputs": [],
   "source": [
    "datast_train = EfficientDataset([vel/vel.mean(), b_xyz/b_xyz.mean(), temp/temp.mean(), np.log10(n_h/n_h.mean()), np.log10(n_e/n_e.mean()), np.log10(n_p/n_p.mean())],\n",
    "                                [pops/pops.sum(axis=-1, keepdims=True)],\n",
    "                                radius_neighbors=1.77)\n",
    "datast_test = EfficientDataset([vel/vel.mean(), b_xyz/b_xyz.mean(), temp/temp.mean(), np.log10(n_h/n_h.mean()), np.log10(n_e/n_e.mean()), np.log10(n_p/n_p.mean())],\n",
    "                                [pops/pops.sum(axis=-1, keepdims=True)],\n",
    "                                radius_neighbors=1.77, split='test')\n",
    "# Get a single sample graph\n",
    "sample_graph = datast_train[0].to(device)\n",
    "\n",
    "# Now, provide the input as a tuple of tensors\n",
    "batch_tensor = torch.zeros(sample_graph.num_nodes, dtype=torch.long).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ef9af76",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "797cf01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.unique(sample_graph.edge_attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "29ccdde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model device:\", next(model.parameters()).device)\n",
    "print(\"sample_graph.x device:\", sample_graph.x.device)\n",
    "print(\"sample_graph.edge_attr device:\", sample_graph.edge_attr.device)\n",
    "print(\"sample_graph.edge_index device:\", sample_graph.edge_index.device)\n",
    "print(\"sample_graph.u device:\", sample_graph.u.device)\n",
    "print(\"batch_tensor device:\", batch_tensor.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c6f3da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "62f69b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "param_size = sum(p.numel() * p.element_size() for p in model.parameters())\n",
    "buffer_size = sum(b.numel() * b.element_size() for b in model.buffers())\n",
    "total_size = param_size + buffer_size\n",
    "\n",
    "print(f\"Model size: {total_size / 1024 ** 2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6863dde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader_train = DataLoader( datast_train, batch_size=10, shuffle=True)\n",
    "loader_test = DataLoader( datast_test, batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c1d6d7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "n_epochs = 50\n",
    "savedir = 'checkpoints/'\n",
    "smooth = 0.05\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Cosine annealing learning rate scheduler. This will reduce the learning rate with a cosing law\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, n_epochs)\n",
    "\n",
    "# Loss function\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# Now start the training\n",
    "train_loss = []\n",
    "valid_loss = []\n",
    "lr = []\n",
    "best_loss = float('inf')\n",
    "\n",
    "time_format = \"%Y.%m.%d-%H:%M:%S\"\n",
    "\n",
    "# print(torch.cuda.memory_summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e92be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, n_epochs + 1):\n",
    "\n",
    "    # filename = str(epoch) #time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    filename = time.strftime(time_format)\n",
    "\n",
    "    # Compute training and validation steps\n",
    "    ################### TRAINING ###################\n",
    "    # Put the model in training mode\n",
    "    model.train()\n",
    "    print(\"\\n\"+\"#\"*80)\n",
    "    print(f\"Epoch {epoch}/{n_epochs}\\nt = {filename}\\nLR = {scheduler.get_last_lr()}\")\n",
    "    # t = tqdm(loader_train)\n",
    "    loss_avg = 0.0\n",
    "\n",
    "    # for batch_idx, (data) in enumerate(t):\n",
    "    for batch_idx, (data) in enumerate(loader_train):\n",
    "\n",
    "        # Extract the node, edges, indices, target, global and batch information from the Data class\n",
    "\n",
    "        # Move them to the GPU\n",
    "        node, edge_attr, edge_index = data.x.to(device), data.edge_attr.to(device), data.edge_index.to(device)\n",
    "        u, batch, target = data.u.to(device), data.batch.to(device), data.y.to(device)\n",
    "\n",
    "        # Reset gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Evaluate Graphnet\n",
    "        out = model(node, edge_attr, edge_index, u, batch)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_fn(out.squeeze(), target.squeeze())\n",
    "\n",
    "        # Compute backpropagation\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        for param_group in optimizer.param_groups:\n",
    "            current_lr = param_group['lr']\n",
    "\n",
    "        # Compute smoothed loss\n",
    "        if (batch_idx == 0):\n",
    "            loss_avg = loss.item()\n",
    "        else:\n",
    "            loss_avg = smooth * loss.item() + (1.0 - smooth) * loss_avg\n",
    "\n",
    "        # free gpu memory\n",
    "        # torch.cuda.empty_cache()\n",
    "\n",
    "    train_loss.append(loss_avg)\n",
    "\n",
    "    ################### VALIDATION ###################\n",
    "    # Do a validation of the model and return the loss\n",
    "\n",
    "    model.eval()\n",
    "    loss_avg = 0\n",
    "    # t = tqdm(loader_test)\n",
    "\n",
    "    mid_time = time.strftime(time_format)\n",
    "    print(f\"Epoch consumed time = {datetime.strptime(mid_time, time_format) - datetime.strptime(filename, time_format)})\")\n",
    "\n",
    "    print(\"Starting the Validation of the epoch:\")\n",
    "    with torch.no_grad():\n",
    "        # for batch_idx, (data) in enumerate(t):\n",
    "        for batch_idx, (data) in enumerate(loader_test):\n",
    "\n",
    "            node, edge_attr, edge_index = data.x.to(device), data.edge_attr.to(device), data.edge_index.to(device)\n",
    "            u, batch, target = data.u.to(device), data.batch.to(device), data.y.to(device)\n",
    "\n",
    "            out = model(node, edge_attr, edge_index, u, batch)\n",
    "\n",
    "            loss = loss_fn(out.squeeze(), target.squeeze())\n",
    "\n",
    "            if (batch_idx == 0):\n",
    "                loss_avg = loss.item()\n",
    "            else:\n",
    "                loss_avg = smooth * loss.item() + (1.0 - smooth) * loss_avg\n",
    "\n",
    "            # t.set_postfix(loss=loss_avg)\n",
    "        print()\n",
    "\n",
    "    valid_loss.append(loss_avg)\n",
    "\n",
    "    finish_time = time.strftime(time_format)\n",
    "    print(f\"full epoch finished {epoch}/{n_epochs} in {datetime.strptime(finish_time, time_format) - datetime.strptime(filename, time_format)} time with loss {loss_avg}\")\n",
    "\n",
    "    # If the validation loss improves, save the model as best\n",
    "    if (valid_loss[-1] < best_loss):\n",
    "        best_loss = valid_loss[-1]\n",
    "\n",
    "        checkpoint = {\n",
    "            'epoch': epoch + 1,\n",
    "            'state_dict': model.state_dict(),\n",
    "            'train_loss': train_loss,\n",
    "            'valid_loss': valid_loss,\n",
    "            'best_loss': best_loss,\n",
    "            'hyperparameters': hyperparameters,\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'lr': scheduler.get_last_lr()\n",
    "        }\n",
    "\n",
    "        print(\"Saving best model...\")\n",
    "        torch.save(checkpoint, savedir + filename + '_best.pth')\n",
    "    lr.append(scheduler.get_last_lr())\n",
    "    # Update the learning rate\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531cf365",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(0, (10,15), dpi=100)\n",
    "plt.plot(train_loss, label='train loss')\n",
    "plt.plot(valid_loss, label='Validation loss')\n",
    "plt.xlabel('Itteration')\n",
    "plt.legend()\n",
    "plt.savefig(savedir + 'loss.pdf')\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
